# Next Word Predictor using GPT-2

This project fine-tunes a GPT-2 transformer model using Hugging Face's `transformers` library on the `wikitext-2` dataset from the `datasets` library.

### Features:
- Tokenization using `GPT2Tokenizer`
- Fine-tuning with `Trainer` API
- Next-word text generation from prompts
- Evaluation using Perplexity (~41.59)

All code is written in Python and follows best practices for tokenizer alignment and language modeling.
