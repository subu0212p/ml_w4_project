{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "lC0SQdN5otdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D6NOFKQ9VKBm"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets evaluate torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eSB8XHRFWUDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F6WDfmXdYrtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pDKO9pWCZuMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and tokenize the dataset"
      ],
      "metadata": {
        "id": "LhcpqCFGo37v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "dataset[\"train\"] = dataset[\"train\"].select(range(1000))\n",
        "dataset[\"validation\"] = dataset[\"validation\"].select(range(200)) #Due to limited GPU memory availability, the training and evaluation datasets were reduced to 1000 and 200 samples respectively.\n",
        "# This allowed the model to train and evaluate successfully without running into out-of-memory issues. The code remains scalable to larger datasets when more compute is available.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xWh1AQsWVLh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data collator and model"
      ],
      "metadata": {
        "id": "It0xB4sUo_cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling, GPT2LMHeadModel\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QP21C1ZPXX-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define evaluation metrics"
      ],
      "metadata": {
        "id": "PxB06jI1pDqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import math\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    shift_logits = torch.tensor(logits[..., :-1, :])\n",
        "    shift_labels = torch.tensor(labels[..., 1:])\n",
        "\n",
        "    loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "    loss = loss_fct(\n",
        "        shift_logits.reshape(-1, shift_logits.size(-1)),\n",
        "        shift_labels.reshape(-1)\n",
        "    )\n",
        "    perplexity = math.exp(loss.item())\n",
        "\n",
        "    top_k = 5\n",
        "    topk_pred = shift_logits.topk(top_k, dim=-1).indices\n",
        "    match = (topk_pred == shift_labels.unsqueeze(-1)).any(-1)\n",
        "    topk_acc = match.float().mean().item()\n",
        "\n",
        "    return {\"perplexity\": perplexity, \"top_5_accuracy\": topk_acc}\n"
      ],
      "metadata": {
        "id": "1sHS9GmPXtPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate with Trainer"
      ],
      "metadata": {
        "id": "bIjjIAyfpIZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-nextword\",\n",
        "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "4r626zDOXzCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the model for prediction"
      ],
      "metadata": {
        "id": "FiokRiqqpOqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Deep learning enables\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=1, do_sample=True)\n",
        "next_word = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(next_word)\n"
      ],
      "metadata": {
        "id": "1Kh4--CCbySf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}